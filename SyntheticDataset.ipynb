{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная версия 5 августа 2020. Все пути захардкожены, в будущем нужно поменять.  \n",
    "Версия 12 августа 2020. Написана функция download_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import wget\n",
    "import pyunpack\n",
    "import fileinput\n",
    "import glob\n",
    "from itertools import chain\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers.tokenization_bert import BasicTokenizer\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDatasetGenerator:\n",
    "    def __init__(self, data_dir='data/'):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset_path = os.path.join(data_dir, 'all_data.txt')\n",
    "        #self.download_data(self.data_dir)\n",
    "        \n",
    "        self.input_num_sentences = None\n",
    "        self.output_num_sentences = None\n",
    "        self.tysya_existing_words = None\n",
    "        self.tsya_existing_words = None\n",
    "        self.n_nn_existing_words = None        \n",
    "        self.load_dics()\n",
    "        self.taglist = ['REPLACE_tsya', 'REPLACE_tysya', 'REPLACE_nn, REPLACE_n', 'REPLACE_techenie', 'REPLACE_techenii']\n",
    "        self.KEEP = 'O'\n",
    "        self.to_file = 'data/dataset.txt'\n",
    "        self.data = None\n",
    "    \n",
    "    def load_dics(self):\n",
    "        print('Loading dictionaries...')\n",
    "        \n",
    "        self.techenie_existing_words = set(['течение', 'течении'])\n",
    "        \n",
    "        with open('data/tsya_vocab.txt', 'r') as f:\n",
    "            pairs = f.read().splitlines()\n",
    "        self.tysya_existing_words = set([pair.split('\\t')[0] for pair in pairs])\n",
    "        self.tsya_existing_words = set([pair.split('\\t')[1] for pair in pairs])\n",
    "        if not os.path.exists('data/all_n_nn_words.txt'):\n",
    "            with open('data/n_words.txt', 'r') as f:\n",
    "                n_words = f.read().splitlines()\n",
    "            n_data = pd.DataFrame(n_words, columns=['text'])\n",
    "            n_data['text'] = n_data['text'].str.replace('\"Словоформа\": \"', '').str.strip('\"').str.strip()\n",
    "            # remove words with capital letters\n",
    "            to_filter = n_data[n_data['text'].str.contains(r'[А-ЯЁ]+', regex=True)]\n",
    "            #  keep those with capital letters which have a hyphen after them\n",
    "            to_append = to_filter[to_filter['text'].str.contains(r'[А-ЯЁ]+-')]\n",
    "            n_data = n_data[~n_data['text'].str.contains(r'[А-ЯЁ]+', regex=True)]\n",
    "            n_data = n_data.append(to_append)\n",
    "            nn_pattern = re.compile(r'\\wнн([аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b', re.IGNORECASE)\n",
    "            n_pattern = re.compile(r'[аоэеиыуёюя]н([аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b', re.IGNORECASE)\n",
    "            nn = n_data[n_data['text'].str.contains(nn_pattern, regex=True)]\n",
    "            n = n_data[n_data['text'].str.contains(n_pattern, regex=True)]\n",
    "            self.n_nn_existing_words = set(nn['text']).union(set(n['text']))\n",
    "            # add word writings with 'е' instead of 'ё'\n",
    "            without_yo = set()\n",
    "            for word in self.n_nn_existing_words:\n",
    "                if word.find('ё') != -1:\n",
    "                    without_yo.add(word.replace('ё', 'е'))\n",
    "            self.n_nn_existing_words.update(without_yo)\n",
    "            # write n_nn_words to disk for further using out of the file\n",
    "            with open('data/all_n_nn_words.txt', 'w') as f:\n",
    "                for line in self.n_nn_existing_words:\n",
    "                    f.write(line + '\\n')\n",
    "        else:\n",
    "            with open('data/all_n_nn_words.txt', 'r') as f:\n",
    "                self.n_nn_existing_words = set(f.read().splitlines())\n",
    "\n",
    "    def download_data(self, data_dir):\n",
    "        print('Downloading data...')\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.mkdir(data_dir)\n",
    "        dataset_name = 'Magazines'\n",
    "        if os.path.exists(self.dataset_path):\n",
    "            raise Exception('all_data.txt already exists')\n",
    "        else:\n",
    "            archive_path = os.path.join(data_dir, dataset_name + '.rar')\n",
    "            if not os.path.exists(archive_path):\n",
    "                archive_url = 'https://linghub.ru/static/Taiga/' + dataset_name +'.rar'\n",
    "                filename = wget.download(archive_url, out=archive_path)\n",
    "            unrared_path = os.path.join(data_dir, dataset_name)\n",
    "            if not os.path.exists(unrared_path):            \n",
    "                pyunpack.Archive(archive_path).extractall(data_dir)\n",
    "            file_list = sorted(glob.glob(os.path.join(unrared_path, 'texts/*.txt')))\n",
    "            with open(self.dataset_path, 'w') as file:\n",
    "                input_lines = fileinput.input(file_list)\n",
    "                file.writelines(input_lines)\n",
    "\n",
    "        n_words_path = os.path.join(data_dir, 'n_words.txt')\n",
    "        if not os.path.exists(n_words_path):\n",
    "            all_orpho_words_path = os.path.join(data_dir, 'db_word_16.06.2020.json')\n",
    "            if not os.path.exists(all_orpho_words_path):\n",
    "                raise FileNotFoundError('Please put db_word_16.06.2020.json in ' + os.path.abspath(data_dir))\n",
    "            with open(all_orpho_words_path, 'r') as f:\n",
    "                with open(n_words_path, 'w') as writefile:\n",
    "                    line = f.readline()\n",
    "                    while line:\n",
    "                        line = f.readline()\n",
    "                        if re.search('н', line) and re.search('Словоформа', line):\n",
    "                            writefile.write(line)\n",
    "        \n",
    "        tsya_vocab_path = os.path.join(data_dir, 'tsya_vocab.txt')\n",
    "        if not os.path.exists(tsya_vocab_path):\n",
    "            raise FileNotFoundError('Please put tsya_vocab.txt in ' + os.path.abspath(data_dir))\n",
    "        \n",
    "    \n",
    "    def select_data(self):\n",
    "        print('Selecting lines larger than 10 symbols...')\n",
    "        with open('data/all_data.txt', 'r') as f:\n",
    "            texts = f.read().splitlines() # 6697245 lines\n",
    "            texts = [' '.join(line.split()) for line in texts if len(line) > 10] # 5014971 lines [5304067 lines]\n",
    "        return texts\n",
    "    \n",
    "    def sentence_tokenize(self, texts):\n",
    "        print('Splitting in sentences...')\n",
    "        sentences = list(chain.from_iterable(list(map(sent_tokenize, texts))))\n",
    "        self.input_num_sentences = len(sentences)\n",
    "        print('Input sentences before splitting on ... :', self.input_num_sentences)\n",
    "        new_sentences = []\n",
    "        for s in sentences:\n",
    "            longer = False\n",
    "            sen = re.split(r'\\.\\.\\.\\s|…\\s', s)\n",
    "            if len(sen) > 1:\n",
    "                longer = True\n",
    "            for i in range(len(sen)):\n",
    "                if longer and i < len(sen) - 1 and len(sen[i]) > 1:\n",
    "                    new_sentences.append(sen[i] + '...')\n",
    "                elif i == len(sen) - 1 and len(sen[i]) > 1:\n",
    "                    new_sentences.append(sen[i])\n",
    "        self.input_num_sentences = len(new_sentences)\n",
    "        print('Input sentences:', self.input_num_sentences)\n",
    "        return pd.DataFrame(new_sentences, columns=['text'])\n",
    "    \n",
    "    def filter_characters(self, data):\n",
    "        print('Filtering characters...')\n",
    "        # Heuristic for correct bert tokenization\n",
    "        data['text'] = data['text'].str.replace(r'[“”«»‘‛’„»«└″′]', \"'\", regex=True)\n",
    "        symbols_to_filter = set()\n",
    "        for i, sent in tqdm(enumerate(data['text'])):\n",
    "            for letter in sent:\n",
    "                if ord(letter) > ord('─'):\n",
    "                    symbols_to_filter.add(letter)\n",
    "        symbols_to_filter.update({'\\u200b','\\u200e', '´', '`'})\n",
    "        symbols_to_filter = ''.join(sym for sym in symbols_to_filter)\n",
    "        filter_regex = re.compile(r'[' + re.escape(symbols_to_filter) + r']')\n",
    "        data['text'] = data['text'].str.replace(filter_regex, '', regex=True)\n",
    "        return data\n",
    "    \n",
    "    def mark_containing_tsya_tysya(self, data):\n",
    "        print('Marking tsya/tysya sentences...')\n",
    "        data['tsya'] = data['text'].str.contains(r'\\wтся\\b', regex=True, flags=re.IGNORECASE)\n",
    "        data['tysya'] = data['text'].str.contains(r'\\wться\\b', regex=True, flags=re.IGNORECASE)\n",
    "        full_tsya_pattern = r'\\b\\w*тся\\b'\n",
    "        full_tysya_pattern = r'\\b\\w*ться\\b'\n",
    "        data['all_tsya_words'] = data[data['tsya']]['text'].str.findall(full_tsya_pattern, flags=re.IGNORECASE)\n",
    "        data['all_tysya_words'] = data[data['tysya']]['text'].str.findall(full_tysya_pattern, flags=re.IGNORECASE)\n",
    "        print(len(data[data['tsya']]), len(data[data['tysya']]))\n",
    "        return data.drop(['tsya', 'tysya'], axis=1)\n",
    "\n",
    "    def mark_containing_n_nn(self, data):\n",
    "        print('Marking n/nn sentences...')\n",
    "        nn_pattern = re.compile(r'\\wнн([аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b', re.IGNORECASE)\n",
    "        n_pattern = re.compile(r'[аоэеиыуёюя]н([аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b', re.IGNORECASE)\n",
    "        data['nn'] = data['text'].str.contains(nn_pattern, regex=True)\n",
    "        data['n'] = data['text'].str.contains(n_pattern, regex=True)\n",
    "        full_n_pattern = r'\\b\\w*[аоэеиыуёюя]н(?:[аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b'\n",
    "        data['all_n_words'] = data[data['n']]['text'].str.findall(full_n_pattern, flags=re.IGNORECASE)\n",
    "        full_nn_pattern = r'\\b\\w*нн(?:[аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b'\n",
    "        data['all_nn_words'] = data[data['nn']]['text'].str.findall(full_nn_pattern, flags=re.IGNORECASE)\n",
    "        print(len(data[data['n']]), len(data[data['nn']]))\n",
    "        return data.drop(['n', 'nn'], axis=1)\n",
    "    \n",
    "    def mark_containing_techenie_ii(self, data):\n",
    "        print('Marking techenie/techenii sentences...')\n",
    "        techenie_pattern = re.compile(r'\\bтечение\\b', re.IGNORECASE)\n",
    "        techenii_pattern = re.compile(r'\\bтечении\\b', re.IGNORECASE)\n",
    "        data['techenie'] = data['text'].str.contains(techenie_pattern, regex=True)\n",
    "        data['techenii'] = data['text'].str.contains(techenii_pattern, regex=True)\n",
    "\n",
    "        data['all_techenie_words'] = data[data['techenie']]['text'].str.findall(r'\\bтечение\\b', flags=re.IGNORECASE)\n",
    "        data['all_techenii_words'] = data[data['techenii']]['text'].str.findall(r'\\bтечении\\b', flags=re.IGNORECASE)\n",
    "        print(len(data[data['techenie']]), len(data[data['techenii']]))\n",
    "        return data.drop(['techenie', 'techenii'], axis=1)\n",
    "    \n",
    "    def generate_one_error_type(self, data, error_name):\n",
    "        print('Generating error', error_name, '...')\n",
    "        if error_name == 'tsya':\n",
    "            pattern_cased = re.compile(r'ТСЯ\\b')\n",
    "            pattern = re.compile(r'тся\\b', re.IGNORECASE)\n",
    "            substitute_cased = 'ТЬСЯ'\n",
    "            substitute = 'ться'\n",
    "            existing_words = self.tysya_existing_words\n",
    "        elif error_name == 'tysya':\n",
    "            pattern_cased = re.compile(r'ТЬСЯ\\b')\n",
    "            pattern = re.compile(r'ться\\b', re.IGNORECASE)\n",
    "            substitute_cased = 'ТСЯ'\n",
    "            substitute = 'тся'\n",
    "            existing_words = self.tsya_existing_words\n",
    "        elif error_name == 'n':\n",
    "            pattern_cased = re.compile(r'(?<=[аоэеиыуёюя])(?-i:Н)(?=([аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b)', re.IGNORECASE)\n",
    "            pattern = re.compile(r'(?<=[аоэеиыуёюя])(?-i:н)(?=([аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b)', re.IGNORECASE)\n",
    "            substitute_cased = 'НН'\n",
    "            substitute = 'нн'\n",
    "            existing_words = self.n_nn_existing_words\n",
    "        elif error_name == 'nn':\n",
    "            pattern_cased = re.compile(r'(?-i:НН)(?=([аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b)', re.IGNORECASE)\n",
    "            pattern = re.compile(r'(?-i:нн)(?=([аоыяеи]|ый|ого|ому|ом|ым|ая|ой|ую|ые|ыми|ых|ое|ою|ий|его|ему|ем|им|яя|ей|ею|юю|ие|ими|их|ее)\\b)', re.IGNORECASE)\n",
    "            substitute_cased = 'Н'\n",
    "            substitute = 'н'\n",
    "            existing_words = self.n_nn_existing_words\n",
    "        elif error_name == 'techenie':\n",
    "            pattern_cased = re.compile(r'\\bТЕЧЕНИЕ\\b', re.IGNORECASE)\n",
    "            pattern = re.compile(r'\\bтечение\\b', re.IGNORECASE)\n",
    "            substitute_cased = 'ТЕЧЕНИИ'\n",
    "            substitute = 'течении'\n",
    "            existing_words = self.techenie_existing_words\n",
    "        elif error_name == 'techenii':\n",
    "            pattern_cased = re.compile(r'\\bТЕЧЕНИИ\\b', re.IGNORECASE)\n",
    "            pattern = re.compile(r'\\bтечении\\b', re.IGNORECASE)\n",
    "            substitute_cased = 'ТЕЧЕНИЕ'\n",
    "            substitute = 'течение'\n",
    "            existing_words = self.techenie_existing_words\n",
    "        original_array = []\n",
    "        changed_array = []\n",
    "        col_name = 'all_' + str(error_name) + '_words'\n",
    "        for line in data[col_name]:\n",
    "            if line is np.nan:\n",
    "                changed_array.append(np.nan)\n",
    "                original_array.append(np.nan)\n",
    "            else:\n",
    "                changed_line = []\n",
    "                original_line = []\n",
    "                for word in line:\n",
    "                    # the order is important! first cased pattern, then pattern\n",
    "                    changed_word = pattern_cased.sub(substitute_cased, word)\n",
    "                    changed_word = pattern.sub(substitute, changed_word)\n",
    "                    if changed_word.lower() in existing_words and changed_word != word:                        \n",
    "                        changed_line.append(changed_word)\n",
    "                        original_line.append(word)\n",
    "                if len(changed_line) == 0:\n",
    "                    changed_array.append(np.nan)\n",
    "                    original_array.append(np.nan)\n",
    "                else:\n",
    "                    changed_array.append(changed_line)\n",
    "                    original_array.append(original_line)\n",
    "        data[str(error_name) + '_words'] = original_array\n",
    "        data[str(error_name) + '_words_errorified'] = changed_array\n",
    "        return data.drop('all_' + str(error_name) + '_words', axis=1)\n",
    "    '''\n",
    "    def filter_data_by_error_set(self, data):\n",
    "        print(len(data[data['tsya_words_errorified'].notna()]))\n",
    "        print(len(data[data['tysya_words_errorified'].notna()]))\n",
    "        print(len(data[data['n_words_errorified'].notna()]))\n",
    "        print(len(data[data['nn_words_errorified'].notna()]))\n",
    "        print(len(data[data['techenie_words_errorified'].notna()]))\n",
    "        print(len(data[data['techenii_words_errorified'].notna()]))\n",
    "        print('Filtering out the sentences that cannot be errorified...')\n",
    "        chosen_ids = set(data[data['n_words_errorified'].notna()].index)\n",
    "        print('Data size:', len(data))\n",
    "        chosen_ids.update(set(data[data['nn_words_errorified'].notna()].index))\n",
    "        print('Data size:', len(data))\n",
    "        chosen_ids.update(set(data[data['tsya_words_errorified'].notna()].index))\n",
    "        print('Data size:', len(data))\n",
    "        chosen_ids.update(set(data[data['tysya_words_errorified'].notna()].index))\n",
    "        print('Data size:', len(data))\n",
    "        chosen_ids.update(set(data[data['techenie_words_errorified'].notna()].index))\n",
    "        print('Data size:', len(data))\n",
    "        chosen_ids.update(set(data[data['techenii_words_errorified'].notna()].index))\n",
    "        print('Data size:', len(data))\n",
    "        self.output_num_sentences = len(chosen_ids)\n",
    "        print('Output sentences:', self.output_num_sentences)\n",
    "        return data.loc[chosen_ids]\n",
    "    '''\n",
    "    def filter_data_by_error_set(self, data):\n",
    "        \n",
    "        print('Filtering out the sentences that cannot be errorified...')\n",
    "        potential_cols = ['tsya_words_errorified', 'tysya_words_errorified', 'n_words_errorified', 'nn_words_errorified', \n",
    "                          'techenie_words_errorified', 'techenii_words_errorified']\n",
    "        \n",
    "        data.dropna(subset=potential_cols, how='all', inplace=True)\n",
    "        self.output_num_sentences = len(data)\n",
    "        print('Output sentences:', self.output_num_sentences)\n",
    "        \n",
    "#         data.to_csv('data_preprocesed.csv')    \n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def errorify_sentences(self, data):\n",
    "        print('Errorifying sentences...')\n",
    "        np.random.seed(42)\n",
    "        # if more than 1 error, they may interfere with each other\n",
    "        # simple setting: only 1 error\n",
    "        potential_cols = ['tsya_words_errorified', 'tysya_words_errorified', 'n_words_errorified', 'nn_words_errorified', \n",
    "                          'techenie_words_errorified', 'techenii_words_errorified']\n",
    "        cols = ['text'] + potential_cols + ['tsya_words', 'tysya_words', 'n_words', 'nn_words', 'techenie_words', 'techenii_words']\n",
    "        potential_data = data[cols].copy()\n",
    "        potential_data['error'] = np.nan\n",
    "        potential_data['error_type'] = np.nan\n",
    "        potential_data['error_word'] = np.nan\n",
    "        potential_data.reset_index(drop=True, inplace=True)\n",
    "        for i, row in tqdm(potential_data.iterrows()): # slow method, to be replaced\n",
    "            #cols_maybe_errors = [col for col in potential_cols if (row[col] and row[col] is not np.nan)]\n",
    "            cols_maybe_errors = [col for col in potential_cols if (row[col] and not np.all(pd.isnull(row[col])))]\n",
    "            error_col = choice(cols_maybe_errors)\n",
    "            # take randomly 1 possible incorrect word or none\n",
    "            # change-in-future: p of 'no error' depends on the number of possible incorrect words\n",
    "            potential_error_words = potential_data.loc[i, error_col]\n",
    "            probabilities = np.empty(len(potential_error_words) + 1)\n",
    "            probabilities[-1] = 0.25\n",
    "            probabilities[:-1] = (1 - probabilities[-1]) / len(potential_error_words)\n",
    "            k = choice(range(len(potential_error_words) + 1), p=probabilities)\n",
    "            if k == len(potential_error_words):\n",
    "                potential_data.loc[i, 'error_type'] = 'none'\n",
    "                potential_data.loc[i, 'error_word'] = np.nan\n",
    "                potential_data.loc[i, 'error'] = potential_data.loc[i, 'text']\n",
    "            else:\n",
    "                potential_data.loc[i, 'error_type'] = error_col.replace('_words_errorified', '')\n",
    "                errorified = potential_data.loc[i, error_col][k]\n",
    "                potential_data.loc[i, 'error_word'] = errorified\n",
    "                original = potential_data.loc[i, error_col.replace('_errorified', '')][k]\n",
    "                potential_data.loc[i, 'error'] = potential_data.loc[i, 'text'].replace(original, errorified)\n",
    "        return potential_data[['text', 'error', 'error_word', 'error_type']]\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return BasicTokenizer(do_lower_case=False).tokenize(sentence)\n",
    "\n",
    "    def tag_in_conll_format(self, data):\n",
    "        data['tag'] = 'REPLACE_' + data['error_type']\n",
    "        exceptions = dict()\n",
    "        column_names = list(data.columns)\n",
    "        fast_data = data.values.tolist()\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for i in tqdm(range(len(fast_data))):\n",
    "            row = fast_data[i]\n",
    "            error_col_id = column_names.index('error')\n",
    "            error_word_col_id = column_names.index('error_word')                \n",
    "            tag_col_id = column_names.index('tag')\n",
    "            sent_tokens = self.tokenize(row[error_col_id])\n",
    "            sent_labels = [self.KEEP] * len(sent_tokens)\n",
    "            if not pd.isnull(row[error_word_col_id]):\n",
    "                error_word = str(row[error_word_col_id])\n",
    "                try:\n",
    "                    error_word_id = sent_tokens.index(error_word)\n",
    "                    sent_labels[error_word_id] = row[tag_col_id]\n",
    "                except Exception as e:\n",
    "                    print(i, e)\n",
    "                    exceptions[i] = row\n",
    "            tokens.append(sent_tokens)\n",
    "            labels.append(sent_labels)\n",
    "        self.output_num_sentences = len(tokens)\n",
    "        return tokens, labels, exceptions\n",
    "    \n",
    "    def write_dataset_to_file(self, tokens, labels):\n",
    "        with open(self.to_file, 'w') as f:\n",
    "            zipped = list(zip(tokens, labels))\n",
    "            for i in range(len(zipped)):\n",
    "                sentence_tokens = zipped[i][0]\n",
    "                sentence_labels = zipped[i][1]\n",
    "                for j in range(len(sentence_tokens)):\n",
    "                    f.write(sentence_tokens[j] + '\\t' + sentence_labels[j] + '\\n')\n",
    "                f.write('\\n')\n",
    "    \n",
    "    def generate(self, to_file=None):\n",
    "#         if to_file is not None:\n",
    "#             self.to_file = to_file\n",
    "#         self.data = self.select_data()\n",
    "#         print('Data size:', len(self.data))\n",
    "# #         with open('data/all_data_longer_than_10.txt') as f:\n",
    "# #             self.data = f.read().splitlines()\n",
    "#         print('Data size:', len(self.data))\n",
    "#         self.data = self.sentence_tokenize(self.data)\n",
    "#         print('Data size:', len(self.data))\n",
    "#         self.data = self.filter_characters(self.data)\n",
    "#         print('Data size:', len(self.data))\n",
    "#         self.data.to_csv('prepared_data.csv')\n",
    "        self.data = pd.read_csv('prepared_data.csv', index_col=0)\n",
    "        print(self.data.info(memory_usage=\"deep\"))\n",
    "        self.data = self.mark_containing_tsya_tysya(self.data)\n",
    "        print('Data size:', len(self.data))\n",
    "        self.data = self.mark_containing_n_nn(self.data)\n",
    "        print('Data size:', len(self.data))\n",
    "        self.data = self.mark_containing_techenie_ii(self.data)\n",
    "        print('Data size:', len(self.data))\n",
    "        for error_name in ['tsya', 'tysya', 'n', 'nn', 'techenie', 'techenii']:\n",
    "            self.data = self.generate_one_error_type(self.data, error_name)\n",
    "            print('Data size:', len(self.data))\n",
    "        self.data = self.filter_data_by_error_set(self.data)\n",
    "        print('Data size:', len(self.data))\n",
    "        self.data = self.errorify_sentences(self.data)\n",
    "        print('Data size:', len(self.data))\n",
    "        tokens, labels, exceptions = self.tag_in_conll_format(self.data)\n",
    "        print('Data size:', len(tokens))\n",
    "        self.data = self.data.drop(exceptions.keys())\n",
    "        self.write_dataset_to_file(tokens, labels)\n",
    "        return self.data, tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data_preprocesed.csv', index_col=0)\n",
    "# data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionaries...\n",
      "CPU times: user 79.5 ms, sys: 9.57 ms, total: 89.1 ms\n",
      "Wall time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generator = SyntheticDatasetGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kristina/IndustrialImmersion/venv_kris/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16135506 entries, 0 to 16135505\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   text    object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.7 GB\n",
      "None\n",
      "Marking tsya/tysya sentences...\n",
      "1306636 593751\n",
      "Data size: 16135506\n",
      "Marking n/nn sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kristina/IndustrialImmersion/venv_kris/lib/python3.7/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4597277 2599852\n",
      "Data size: 16135506\n",
      "Marking techenie/techenii sentences...\n",
      "13796 484\n",
      "Data size: 16135506\n",
      "Generating error tsya ...\n",
      "Data size: 16135506\n",
      "Generating error tysya ...\n",
      "Data size: 16135506\n",
      "Generating error n ...\n",
      "Data size: 16135506\n",
      "Generating error nn ...\n",
      "Data size: 16135506\n",
      "Generating error techenie ...\n",
      "Data size: 16135506\n",
      "Generating error techenii ...\n",
      "Data size: 16135506\n",
      "Filtering out the sentences that cannot be errorified...\n",
      "Output sentences: 1069464\n",
      "Data size: 1069464\n",
      "Errorifying sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1069464it [12:53, 1382.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1069464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 612449/1069464 [01:35<01:02, 7347.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611106 'манны' is not in list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1069464/1069464 [02:46<00:00, 6421.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1069464\n",
      "CPU times: user 21min 58s, sys: 19.1 s, total: 22min 17s\n",
      "Wall time: 22min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generated_data, tokens, labels = generator.generate(to_file='data/dataset_plus_correct.txt')\n",
    "# Input sentences should be 15664256 before sentence_tokenize function: correct\n",
    "# Input sentences should be 16135506: correct\n",
    "# Output sentences should be 1057067: correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Temporarily Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cbfef74782d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marchive_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://bit.ly/2pvhWZm'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'news'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/wget.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mbinurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mulib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 503: Service Temporarily Unavailable"
     ]
    }
   ],
   "source": [
    "archive_url = 'http://bit.ly/2pvhWZm'\n",
    "filename = wget.download(archive_url, os.path.join('./data', 'news' + '.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyunpack.Archive('./data/news.zip').extractall('./data')\n",
    "# file_list = sorted(glob.glob(os.path.join(unrared_path, 'texts/*.txt')))\n",
    "# with open(self.dataset_path, 'w') as file:\n",
    "#     input_lines = fileinput.input(file_list)\n",
    "#     file.writelines(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kristina/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1069463, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n           307414\n",
       "none        267100\n",
       "tysya       184171\n",
       "tsya        178970\n",
       "nn          121851\n",
       "techenie      9633\n",
       "techenii       324\n",
       "Name: error_type, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data['error_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2497515108049554"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(generated_data['error_type'] == 'none').sum() / len(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>error</th>\n",
       "      <th>error_word</th>\n",
       "      <th>error_type</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В недавней вашей прозе о Слуцком* вы так описываете его поэтику: 'хриплое клокотание'; 'страшная и обыденная жизнь и смерть, страшно обыденная и обыденно страшная'; 'разговорная речь с вкрапленьями профессионального и бытового жаргона'; 'небрежный (как бы) или иронический тон высказывания и тут же рядом — речь ораторская, поддержанная высокой архаикой вплоть до церковнославянизмов'; 'речевой сплав горнего и дольнего' — и далее, как бы итожа: 'преткновенная гармония', 'лиро-эпос'...</td>\n",
       "      <td>В недавней вашей прозе о Слуцком* вы так описываете его поэтику: 'хриплое клокотание'; 'страшная и обыденная жизнь и смерть, страшно обыденная и обыденно страшная'; 'разговорная речь с вкрапленьями профессионального и бытового жаргонна'; 'небрежный (как бы) или иронический тон высказывания и тут же рядом — речь ораторская, поддержанная высокой архаикой вплоть до церковнославянизмов'; 'речевой сплав горнего и дольнего' — и далее, как бы итожа: 'преткновенная гармония', 'лиро-эпос'...</td>\n",
       "      <td>жаргонна</td>\n",
       "      <td>n</td>\n",
       "      <td>REPLACE_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Когда Лермонтов написал 'Есть речи — значенье Темно иль ничтожно...' — он-то написал это не ничтожно, а осмысленно.</td>\n",
       "      <td>Когда Лермонтов написал 'Есть речи — значенье Темно иль ничтожно...' — он-то написал это не ничтожно, а осмысленно.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>REPLACE_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>О. Ч. К любым языковым формулам надо относиться осторожно, хотя и сказано энергично.</td>\n",
       "      <td>О. Ч. К любым языковым формулам надо относится осторожно, хотя и сказано энергично.</td>\n",
       "      <td>относится</td>\n",
       "      <td>tysya</td>\n",
       "      <td>REPLACE_tysya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Для кого-то так относиться к замыслу важно — тому же Бродскому, который писал достаточно протяженные, синтаксически развернутые вещи.</td>\n",
       "      <td>Для кого-то так относится к замыслу важно — тому же Бродскому, который писал достаточно протяженные, синтаксически развернутые вещи.</td>\n",
       "      <td>относится</td>\n",
       "      <td>tysya</td>\n",
       "      <td>REPLACE_tysya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Картина хороша, но это жанр.</td>\n",
       "      <td>Картинна хороша, но это жанр.</td>\n",
       "      <td>Картинна</td>\n",
       "      <td>n</td>\n",
       "      <td>REPLACE_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069459</th>\n",
       "      <td>Душно в комнате было, и на стене картина, как форточка, открывалась,</td>\n",
       "      <td>Душно в комнате было, и на стене картина, как форточка, открывалась,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>REPLACE_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069460</th>\n",
       "      <td>Посмотри – все дробится в его узоре, и на самой границе небес и вод обнажается детский затылок моря, закругляется горизонт.</td>\n",
       "      <td>Посмотри – все дробится в его узоре, и на самой границе небес и вод обнажается детский затылок моря, закругляется горизонт.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>REPLACE_none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069461</th>\n",
       "      <td>Иногда асфальт становится на дыбы, корни и грибы растут из него.</td>\n",
       "      <td>Иногда асфальт становиться на дыбы, корни и грибы растут из него.</td>\n",
       "      <td>становиться</td>\n",
       "      <td>tsya</td>\n",
       "      <td>REPLACE_tsya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069462</th>\n",
       "      <td>Вымерзают глубины до дна.</td>\n",
       "      <td>Вымерзают глубинны до дна.</td>\n",
       "      <td>глубинны</td>\n",
       "      <td>n</td>\n",
       "      <td>REPLACE_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069463</th>\n",
       "      <td>Обездвижены выси и воды, вымерзают глубины до дна.</td>\n",
       "      <td>Обездвижены выси и воды, вымерзают глубинны до дна.</td>\n",
       "      <td>глубинны</td>\n",
       "      <td>n</td>\n",
       "      <td>REPLACE_n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1069463 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "0        В недавней вашей прозе о Слуцком* вы так описываете его поэтику: 'хриплое клокотание'; 'страшная и обыденная жизнь и смерть, страшно обыденная и обыденно страшная'; 'разговорная речь с вкрапленьями профессионального и бытового жаргона'; 'небрежный (как бы) или иронический тон высказывания и тут же рядом — речь ораторская, поддержанная высокой архаикой вплоть до церковнославянизмов'; 'речевой сплав горнего и дольнего' — и далее, как бы итожа: 'преткновенная гармония', 'лиро-эпос'...   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                           Когда Лермонтов написал 'Есть речи — значенье Темно иль ничтожно...' — он-то написал это не ничтожно, а осмысленно.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                          О. Ч. К любым языковым формулам надо относиться осторожно, хотя и сказано энергично.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                         Для кого-то так относиться к замыслу важно — тому же Бродскому, который писал достаточно протяженные, синтаксически развернутые вещи.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Картина хороша, но это жанр.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...   \n",
       "1069459                                                                                                                                                                                                                                                                                                                                                                                                                                    Душно в комнате было, и на стене картина, как форточка, открывалась,   \n",
       "1069460                                                                                                                                                                                                                                                                                                                                                                             Посмотри – все дробится в его узоре, и на самой границе небес и вод обнажается детский затылок моря, закругляется горизонт.   \n",
       "1069461                                                                                                                                                                                                                                                                                                                                                                                                                                        Иногда асфальт становится на дыбы, корни и грибы растут из него.   \n",
       "1069462                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Вымерзают глубины до дна.   \n",
       "1069463                                                                                                                                                                                                                                                                                                                                                                                                                                                      Обездвижены выси и воды, вымерзают глубины до дна.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           error  \\\n",
       "0        В недавней вашей прозе о Слуцком* вы так описываете его поэтику: 'хриплое клокотание'; 'страшная и обыденная жизнь и смерть, страшно обыденная и обыденно страшная'; 'разговорная речь с вкрапленьями профессионального и бытового жаргонна'; 'небрежный (как бы) или иронический тон высказывания и тут же рядом — речь ораторская, поддержанная высокой архаикой вплоть до церковнославянизмов'; 'речевой сплав горнего и дольнего' — и далее, как бы итожа: 'преткновенная гармония', 'лиро-эпос'...   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                            Когда Лермонтов написал 'Есть речи — значенье Темно иль ничтожно...' — он-то написал это не ничтожно, а осмысленно.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                            О. Ч. К любым языковым формулам надо относится осторожно, хотя и сказано энергично.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                           Для кого-то так относится к замыслу важно — тому же Бродскому, который писал достаточно протяженные, синтаксически развернутые вещи.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Картинна хороша, но это жанр.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...   \n",
       "1069459                                                                                                                                                                                                                                                                                                                                                                                                                                     Душно в комнате было, и на стене картина, как форточка, открывалась,   \n",
       "1069460                                                                                                                                                                                                                                                                                                                                                                              Посмотри – все дробится в его узоре, и на самой границе небес и вод обнажается детский затылок моря, закругляется горизонт.   \n",
       "1069461                                                                                                                                                                                                                                                                                                                                                                                                                                        Иногда асфальт становиться на дыбы, корни и грибы растут из него.   \n",
       "1069462                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Вымерзают глубинны до дна.   \n",
       "1069463                                                                                                                                                                                                                                                                                                                                                                                                                                                      Обездвижены выси и воды, вымерзают глубинны до дна.   \n",
       "\n",
       "          error_word error_type            tag  \n",
       "0           жаргонна          n      REPLACE_n  \n",
       "1                NaN       none   REPLACE_none  \n",
       "2          относится      tysya  REPLACE_tysya  \n",
       "3          относится      tysya  REPLACE_tysya  \n",
       "4           Картинна          n      REPLACE_n  \n",
       "...              ...        ...            ...  \n",
       "1069459          NaN       none   REPLACE_none  \n",
       "1069460          NaN       none   REPLACE_none  \n",
       "1069461  становиться       tsya   REPLACE_tsya  \n",
       "1069462     глубинны          n      REPLACE_n  \n",
       "1069463     глубинны          n      REPLACE_n  \n",
       "\n",
       "[1069463 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ошибки, которые мы взяли в работу\n",
    "\n",
    "1. Заменить a на b:\n",
    "    - -ться <-> -тся: просто ищем тся **[РЕАЛИЗОВАНО]**\n",
    "    - н <-> нн (совершенно, длина): н/нн + список окончаний (см. ниже) **[РЕАЛИЗОВАНО]**\n",
    "    - течение <-> течении: просто ищем\n",
    "    - ни- <-> не-: не-ни в начале слова\n",
    "2. Вставить/убрать пробел: имеет ли смысл в рандомных местах? Наверное, не очень пока\n",
    "    - ни * <-> ни* (никто)\n",
    "    - то же <-> тоже\n",
    "    - что бы <-> чтобы (и чтоб)\n",
    "    - бестолку <-> бес толку\n",
    "    - (сложные слова) мега масштабы <-> мегамасштабы\n",
    "3. Вставить/убрать дефис:\n",
    "    - из-за <-> из за\n",
    "    - *-то <-> * то\n",
    "    - *-таки <-> * таки\n",
    "4. Комбинация (1) и (2):\n",
    "    - не кто <-> никто"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ошибки, которые мы пока не взяли в работу:\n",
    "1. Опечатки: вставить/заменить букву\n",
    "    - ве(ч)ером, в(а)ши, бес колебаний, белее того, без везти, ...\n",
    "2. Вставить предлог\n",
    "    - письмо (с) фотографиями, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging scheme\n",
    "\n",
    "#### Replace operations\n",
    "\n",
    "- replace_tsya\n",
    "- replace_tysya\n",
    "- replace_n\n",
    "- replace_nn\n",
    "- replace_techenii\n",
    "- replace_techenie\n",
    "- replace_ni\n",
    "- replace_ne\n",
    "\n",
    "#### Merge and Split operations\n",
    "\n",
    "- merge: ни кто -> никто\n",
    "- split: не весел -> невесел\n",
    "- merge_hyphen: из за -> из-за\n",
    "- split_hyphen: я-таки -> я таки\n",
    "- merge_replace_ni: не кто -> никто\n",
    "- merge_replace_ne:\n",
    "- split_replace_ni:\n",
    "- split_replace_ne: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Списки окончаний\n",
    "\n",
    "1. **Список окончаний прилагательных и причастий с н/нн**:  \n",
    "-ый, -ого, -ому, -ом, -ым, -ая, -ой, -ую, -ые, -ыми, -ых, -а, -о, -ы  \n",
    "На всякий случай мягкий вариант (лингвисты сказали не делать, но, может, всё-таки добавить на всякий случай?):  \n",
    "ий|его|ему|ем|им|яя|ей|юю|ие|ими|их|я|е|и  \n",
    "и пустое окончание, но нам оно не нужно, потому что там гласный вставляется: \"пустынен\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
